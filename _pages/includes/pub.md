# ðŸ“š Publications
\* Eauql contribution. \# Corresponding author

## Preprint

<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><img src='images/FineCLIP/FineCLIP_Performance.png' alt="sym" width="100%"  >
</div></div>
<div class='paper-box-text' markdown="1">

[Align Before Segment: Understanding Visual Encoder Fine-tuning for Open Vocabulary Segmentation
]()

**Yunheng Li**, Quansheng Zeng, Zhong-Yu Li, Enguang Wang, Qibin Hou\#,
Ming-Ming Cheng

FineCLIP is an align-before-segment framework that fine-tunes CLIP with dense image-text alignment, notably enhancing open-vocabulary segmentation performance.

<!-- [[**Paper**]](https://arxiv.org/pdf/2312.05830)
[[**Code**]](https://github.com/lyhisme/DeST)  -->


</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"></div><img src='images/DeST/DeST_FLOPS_Framework.png' alt="sym" width="100%"  >
</div></div>
<div class='paper-box-text' markdown="1">

[A Decoupled Spatio-Temporal Framework for Skeleton-based Action Segmentation
](https://arxiv.org/pdf/2312.05830)


**Yunheng Li**, Zhong-Yu Li, Shanghua Gao, Qilong Wang, Qibin Hou\#,
Ming-Ming Cheng

[[**Paper**]](https://arxiv.org/pdf/2312.05830)
[[**Code**]](https://github.com/lyhisme/DeST) 

<!-- DeST adopts decoupled spatio-temporal interaction and joint-decoupled temporal modeling to decouple the cascaded spatio-temporal interaction and learn the discriminative motion pattern of each joint, respectively -->

Decoupled Spatio-Temporal (DeST) framework is the first to decouple spatio-temporal modeling for effective skeleton-based action segmentation.

<!-- IDT-GCN employs an Involving Distinction Graph Convolutional Network (ID-GC) to model the relationships between different actions through multiple adaptive topologies. 
This approach effectively captures both similar and divergent dependencies among spatial joints. 
Additionally, Temporal Segment Regression (TSR) is used to extract segmented encoding features and action boundary representations by modeling action sequences. -->

</div>
</div>

## Journal

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">IEEE TCSVT 2023</div><img src='images/TCSVT-IDT-GCN/IDT-GCN_Framework.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Involving Distinguished Temporal Graph Convolutional Networks for Skeleton-Based Temporal Action Segmentation
](https://ieeexplore.ieee.org/abstract/document/10148994)

**Yunheng Li**, Kai-Yuan Liu, Sheng-Lan Liu\#, Lin Feng, Hong Qiao

[[**Paper**]](https://ieeexplore.ieee.org/abstract/document/10148994) 


IDT-GCN employs an Involving Distinction Graph Convolutional Network (ID-GC) to effectively capture both similar and differential dependencies among spatial joints through multiple adaptive topologies. 
Additionally, Temporal Segment Regression (TSR) is used to model action sequences.

<!-- IDT-GCN employs an Involving Distinction Graph Convolutional Network (ID-GC) to model the relationships between different actions through multiple adaptive topologies. 
This approach effectively captures both similar and differential dependencies among spatial joints. 
Additionally, Temporal Segment Regression (TSR) is used to extract segmented encoding features and action boundary representations by modeling action sequences. -->

</div>
</div>

## Conference

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICML 2024</div><img src='images/ICML24_Cascade-CLIP/Cascade-CLIP_Framework.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Cascade-CLIP: Cascaded Vision-Language Embeddings Alignment for Zero-Shot Semantic Segmentation
](https://arxiv.org/pdf/2406.00670)

**Yunheng Li**, Zhong-Yu Li, Quansheng Zeng, Qibin Hou\#, Ming-Ming Cheng

[[**Paper**]](https://arxiv.org/pdf/2406.00670) 
[[**Code**]](https://github.com/HVision-NKU/Cascade-CLIP) 
[[**é›†æ™ºä¹¦ç«¥**]](https://mp.weixin.qq.com/s/skF54m3QvVNlxwxWG_XKuA) 
[[**Poster**]](https://icml.cc/virtual/2024/poster/33865) 

<!-- <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->

Cascade-CLIP aligns vision-language embeddings via **cascaded manner**, effectively leveraging CLIP's **multi-level visual features** for better zero-shot segmentation.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2023</div><img src='images/CVPR2023_D-TSTAS/D-TSTAS_Framework.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Reducing the Label Bias for Timestamp Supervised Temporal Action Segmentation](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Reducing_the_Label_Bias_for_Timestamp_Supervised_Temporal_Action_Segmentation_CVPR_2023_paper.pdf)

Kaiyuan Liu\*, **Yunheng Li\***, Shenglan Liu\#, Chenwei Tan, Zihang Shao

[[**Paper**]](https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_Reducing_the_Label_Bias_for_Timestamp_Supervised_Temporal_Action_Segmentation_CVPR_2023_paper.pdf) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> [[**Slides**]](https://cvpr.thecvf.com/media/cvpr-2023/Slides/21372.pdf) [[**Poster**]](https://cvpr.thecvf.com/virtual/2023/poster/21372) 
<!-- <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong> -->

D-TSTAS employs a masked timestamp prediction method to **reduce dependency on timestamps** and a center-oriented timestamp expansion technique to **capture semantic-rich motion representations**.


</div>
</div>



# ðŸ“ƒ Others


[Spatial Focus Attention for Fine-grained Skeleton-based Action Tasks.](https://ieeexplore.ieee.org/abstract/document/9860042)
**IEEE SPL**, ***2022***. 
Kaiyuan Liu, **Yunheng Li**, Yuanfeng Xu, et al.
[[**Paper**]](https://ieeexplore.ieee.org/abstract/document/9860042) 

[Double Attention Network Based on Sparse Sampling.](https://www.computer.org/csdl/proceedings-article/icme/2022/09859819/1G9EAL2K8JG)
**IEEE ICME**, ***2022***. 
Zhuben Dong, **Yunheng Li**, Yiwei Sun, et al.
[[**Paper**]](https://www.computer.org/csdl/proceedings-article/icme/2022/09859819/1G9EAL2K8JG) 

[Eicient Two-Step Networks for Temporal Action Segmentation.](https://www.sciencedirect.com/science/article/abs/pii/S0925231221006998)
**Neurocomputing**, ***2021***. 
**Yunheng Li**, Zhuben Dong, Kaiyuan Liu, et al.
[[**Paper**]](https://www.sciencedirect.com/science/article/abs/pii/S0925231221006998) 
[[**Code**]](https://github.com/lyhisme/ETSN) 

[Temporal Segmentation of Fine-gained Semantic Action: A Motion-centered Figure Skating Dataset.](https://www.sciencedirect.com/science/article/abs/pii/S0925231221006998)
**AAAI**, ***2021***. 
Shenglan Liu, Aibin Zhang\*, **Yunheng Li\***, et al.
[[**Paper**]](https://ojs.aaai.org/index.php/AAAI/article/view/16314) 
[[**Datasets**]](https://shenglanliu.github.io/mcfs-dataset/) 


